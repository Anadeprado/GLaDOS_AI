{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c788d810",
   "metadata": {},
   "source": [
    "## Follow with HAARCASCADE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "692fd96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Cargar clasificador Haar para detección de cuerpo o cara\n",
    "# Usa uno u otro dependiendo de lo que quieras detectar\n",
    "# cascade_path = cv2.data.haarcascades + \"haarcascade_fullbody.xml\"\n",
    "# cascade_path = cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\"\n",
    "cascade_path = cv2.data.haarcascades + \"haarcascade_upperbody.xml\"\n",
    "\n",
    "detector = cv2.CascadeClassifier(cascade_path)\n",
    "\n",
    "# Iniciar cámara\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Umbral para decidir si girar\n",
    "THRESHOLD = 50  # píxeles\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    detections = detector.detectMultiScale(frame_gray, scaleFactor=1.1, minNeighbors=5)\n",
    "\n",
    "    frame_center_x = frame.shape[1] // 2\n",
    "    direction = \"Centro\"\n",
    "\n",
    "    for (x, y, w, h) in detections:\n",
    "        # Dibujar la detección\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "        # Calcular el centro del objeto detectado\n",
    "        person_center_x = x + w // 2\n",
    "\n",
    "        # Decidir hacia dónde mover la cámara\n",
    "        offset = person_center_x - frame_center_x\n",
    "        if offset > THRESHOLD:\n",
    "            direction = \"Girar Derecha\"\n",
    "        elif offset < -THRESHOLD:\n",
    "            direction = \"Girar Izquierda\"\n",
    "        else:\n",
    "            direction = \"Centro\"\n",
    "\n",
    "        # Solo usamos la primera detección para simplificar\n",
    "        break\n",
    "\n",
    "    # Mostrar dirección sobre la imagen\n",
    "    cv2.putText(frame, f\"Direccion: {direction}\", (10, 30),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "\n",
    "    # Mostrar imagen\n",
    "    cv2.imshow(\"Detección\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5999d9",
   "metadata": {},
   "source": [
    "## Follow with YOLOv4-Tiny"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9cd387",
   "metadata": {},
   "source": [
    "Ejemplo ejecutar YOLO simple sin GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20167fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Cargar clases desde coco.names\n",
    "with open(\"coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Cargar red YOLOv4-Tiny\n",
    "net = cv2.dnn.readNetFromDarknet(\"yolov4-tiny.cfg\", \"yolov4-tiny.weights\")\n",
    "net.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)  # Usar CPU\n",
    "net.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)\n",
    "\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers().flatten()]\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    height, width = frame.shape[:2]\n",
    "\n",
    "    # Preparar blob\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1/255.0, (416, 416), swapRB=True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    outputs = net.forward(output_layers)\n",
    "\n",
    "    person_detected = False\n",
    "    direction = \"Centro\"\n",
    "\n",
    "    for output in outputs:\n",
    "        for detection in output:\n",
    "            scores = detection[5:]\n",
    "            class_id = int(scores.argmax())\n",
    "            confidence = scores[class_id]\n",
    "\n",
    "            if confidence > 0.5 and classes[class_id] == \"person\":\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width)\n",
    "                h = int(detection[3] * height)\n",
    "\n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "\n",
    "                cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "                cv2.putText(frame, \"Persona\", (x, y - 10),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "                # Lógica de centrado\n",
    "                frame_center_x = width // 2\n",
    "                offset = center_x - frame_center_x\n",
    "\n",
    "                if offset > 50:\n",
    "                    direction = \"Girar Derecha\"\n",
    "                elif offset < -50:\n",
    "                    direction = \"Girar Izquierda\"\n",
    "                else:\n",
    "                    direction = \"Centro\"\n",
    "\n",
    "                person_detected = True\n",
    "                break\n",
    "        if person_detected:\n",
    "            break\n",
    "\n",
    "    # Mostrar dirección\n",
    "    cv2.putText(frame, f\"Direccion: {direction}\", (10, 30),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "\n",
    "    cv2.imshow(\"YOLOv4-Tiny Detección\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615ca30f",
   "metadata": {},
   "source": [
    "- Solo detectar una sola persona (la primera que encuentre).\n",
    "- Dibujar el bounding box solo para esa persona.\n",
    "- Ejecutar YOLO solo cada X fotogramas (para ahorrar CPU).\n",
    "- Dibuja una señal de \"objetivo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "196a106a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Configuración de la detección\n",
    "FRAME_SKIP = 10 # Sólo detectar cada X frames para mejorar rendimiento\n",
    "CONFIDENCE_THRESHOLD = 0.5\n",
    "\n",
    "# Función para dibujar un overlay de objetivo en la imagen\n",
    "def draw_target_overlay(frame, center_x, center_y):\n",
    "    # Parámetros de color y grosor\n",
    "    color = (0, 0, 255)  # Rojo en BGR\n",
    "    thickness = 2\n",
    "\n",
    "    # Círculos concéntricos\n",
    "    # for radius in [15, 30, 45]:\n",
    "    for radius in [10, 20, 30]:\n",
    "        cv2.circle(frame, (center_x, center_y), radius, color, thickness)\n",
    "\n",
    "    # Línea vertical\n",
    "    cv2.line(frame, (center_x, center_y - 50), (center_x, center_y + 50), color, thickness)\n",
    "    # Línea horizontal\n",
    "    cv2.line(frame, (center_x - 50, center_y), (center_x + 50, center_y), color, thickness)\n",
    "\n",
    "\n",
    "# Cargar clases desde coco.names\n",
    "with open(\"coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Cargar red YOLOv4-Tiny\n",
    "net = cv2.dnn.readNetFromDarknet(\"yolov4-tiny.cfg\", \"yolov4-tiny.weights\")\n",
    "net.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)  # Usar CPU\n",
    "net.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)\n",
    "\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers().flatten()]\n",
    "\n",
    "\n",
    "# Captura de video\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "person_box = None\n",
    "\n",
    "frame_count = 0\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame_count += 1\n",
    "    \n",
    "    # Procesar cada FRAME_SKIP frames\n",
    "    if frame_count % FRAME_SKIP == 0:\n",
    "\n",
    "        height, width = frame.shape[:2]\n",
    "        person_box = None\n",
    "        center_x = int(width // 2)\n",
    "        center_y = int(height // 2)\n",
    "\n",
    "        # Preprocesamiento\n",
    "        blob = cv2.dnn.blobFromImage(frame, 1/255.0, (416, 416), swapRB=True, crop=False)\n",
    "        net.setInput(blob)\n",
    "        outputs = net.forward(output_layers)\n",
    "\n",
    "        for output in outputs:\n",
    "            for detection in output:\n",
    "                scores = detection[5:]\n",
    "                class_id = int(scores.argmax())\n",
    "                confidence = scores[class_id]\n",
    "\n",
    "                if classes[class_id] == \"person\" and confidence > CONFIDENCE_THRESHOLD:\n",
    "                    center_x = int(detection[0] * width)\n",
    "                    center_y = int(detection[1] * height)\n",
    "                    w = int(detection[2] * width)\n",
    "                    h = int(detection[3] * height)\n",
    "\n",
    "                    x = int(center_x - w / 2)\n",
    "                    y = int(center_y - h / 2)\n",
    "                    person_box = (x, y, w, h)\n",
    "                    break  # Detiene el bucle tras la primera persona detectada\n",
    "            if person_box:\n",
    "                break  # Sal de los outputs\n",
    "\n",
    "    # Dibuja el cuadro si hay una persona detectada\n",
    "    if person_box is not None:\n",
    "        x, y, w, h = person_box\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "        cv2.putText(frame, \"STUPID HUMAN\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "        # Dibuja el objetivo de la cámara (centro de la imagen o centro de la persona si se ha detectado). Pinta un punto rojo \n",
    "        # cv2.circle(frame, (center_x, center_y), 5, (0, 0, 255), -1)\n",
    "        draw_target_overlay(frame, center_x, center_y)\n",
    "\n",
    "    cv2.imshow(\"Detection\", frame)\n",
    "    if cv2.waitKey(1) == 27:  # ESC para salir\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca85e7bf",
   "metadata": {},
   "source": [
    "Enviar comando movimiento Smooth a los servos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7af655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# # import serial ## UNCOMMENT THIS LINE IF YOU WANT TO USE ARDUINO\n",
    "# import time\n",
    "# import numpy as np\n",
    "\n",
    "# # Configura el puerto serial para Arduino\n",
    "# # arduino = serial.Serial('COM3', 9600)  # Cambia por tu puerto ## UNCOMMENT THIS LINE IF YOU WANT TO USE ARDUINO\n",
    "# time.sleep(2)\n",
    "\n",
    "# # Carga nombres de clases\n",
    "# with open(\"coco.names\", \"r\") as f:\n",
    "#     classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# # Carga modelo YOLOv4-Tiny\n",
    "# net = cv2.dnn.readNet(\"yolov4-tiny.weights\", \"yolov4-tiny.cfg\")\n",
    "# net.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\n",
    "# net.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)\n",
    "\n",
    "# layer_names = net.getLayerNames()\n",
    "# output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers().flatten()]\n",
    "\n",
    "# cap = cv2.VideoCapture(0)\n",
    "\n",
    "# frame_count = 0\n",
    "\n",
    "# # Configuración de la detección\n",
    "# DETECT_EVERY_N_FRAMES = 10 # Sólo detectar cada X frames para mejorar rendimiento\n",
    "# CONFIDENCE_THRESHOLD = 0.5\n",
    "\n",
    "# # Variables para suavizado\n",
    "# last_center_x = None\n",
    "# last_center_y = None\n",
    "# smoothing_factor = 0.2  # entre 0 y 1, menor es más lento\n",
    "\n",
    "# def map_range(value, leftMin, leftMax, rightMin, rightMax):\n",
    "#     leftSpan = leftMax - leftMin\n",
    "#     rightSpan = rightMax - rightMin\n",
    "#     valueScaled = float(value - leftMin) / float(leftSpan)\n",
    "#     return rightMin + (valueScaled * rightSpan)\n",
    "\n",
    "# while True:\n",
    "#     ret, frame = cap.read()\n",
    "#     if not ret:\n",
    "#         break\n",
    "\n",
    "#     height, width, _ = frame.shape\n",
    "#     center_x = None\n",
    "#     center_y = None\n",
    "\n",
    "#     frame_count += 1\n",
    "#     if frame_count % DETECT_EVERY_N_FRAMES == 0:\n",
    "#         # Crear blob y pasar por la red\n",
    "#         blob = cv2.dnn.blobFromImage(frame, 1/255.0, (416,416), swapRB=True, crop=False)\n",
    "#         net.setInput(blob)\n",
    "#         outs = net.forward(output_layers)\n",
    "\n",
    "#         for out in outs:\n",
    "#             for detection in out:\n",
    "#                 scores = detection[5:]\n",
    "#                 class_id = np.argmax(scores)\n",
    "#                 confidence = scores[class_id]\n",
    "#                 if confidence > CONFIDENCE_THRESHOLD and classes[class_id] == \"person\":\n",
    "#                     centerX = int(detection[0] * width)\n",
    "#                     centerY = int(detection[1] * height)\n",
    "#                     w = int(detection[2] * width)\n",
    "#                     h = int(detection[3] * height)\n",
    "#                     x = int(centerX - w / 2)\n",
    "#                     y = int(centerY - h / 2)\n",
    "#                     person_box = [x, y, w, h]\n",
    "\n",
    "#         # Non max suppression para eliminar detecciones solapadas\n",
    "#         indexes = cv2.dnn.NMSBoxes(person_boxes, confidences, 0.5, 0.4)\n",
    "\n",
    "#         if len(indexes) > 0:\n",
    "#             # Tomamos la detección con mayor confianza (primer índice)\n",
    "#             i = indexes[0]\n",
    "#             x, y, w, h = person_boxes[i]\n",
    "#             center_x = x + w // 2\n",
    "#             center_y = y + h // 2\n",
    "#             # Dibuja el cuadro si hay una persona detectada\n",
    "#             cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "#             cv2.putText(frame, \"STUPID HUMAN\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "#     # Suavizado de la posición de la cara detectada\n",
    "#     if center_x is not None and center_y is not None:\n",
    "#         if last_center_x is None or last_center_y is None:\n",
    "#             smoothed_x = center_x\n",
    "#             smoothed_y = center_y\n",
    "#         else:\n",
    "#             smoothed_x = int(last_center_x + smoothing_factor * (center_x - last_center_x))\n",
    "#             smoothed_y = int(last_center_y + smoothing_factor * (center_y - last_center_y))\n",
    "\n",
    "#         last_center_x, last_center_y = smoothed_x, smoothed_y\n",
    "\n",
    "#         # Mapear la posición a ángulos servo\n",
    "#         servo_1_angle = int(map_range(smoothed_x, 0, width, 0, 180))\n",
    "#         servo_2_angle = int(map_range(smoothed_y, 0, height, 0, 180))\n",
    "\n",
    "#         command = f\"{servo_1_angle},{servo_2_angle}\\n\"\n",
    "#         # arduino.write(command.encode()) ## UNCOMMENT THIS LINE IF YOU WANT TO USE ARDUINO\n",
    "\n",
    "#         # Dibujar detección y punto centrado suavizado\n",
    "#         cv2.putText(frame, f\"Servo1: {servo_1_angle}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,255,0), 2)\n",
    "#         cv2.putText(frame, f\"Servo2: {servo_2_angle}\", (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,255,0), 2)\n",
    "#         draw_target_overlay(frame, smoothed_x, smoothed_y)\n",
    "\n",
    "#     cv2.imshow(\"YOLO Person Tracking\", frame)\n",
    "\n",
    "#     if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#         break\n",
    "\n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()\n",
    "# # arduino.close() ## UNCOMMENT THIS LINE IF YOU WANT TO USE ARDUINO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59c2910",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (lab)",
   "language": "python",
   "name": "lab-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
